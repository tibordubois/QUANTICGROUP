\section{Searching for an Unknown Number of Solutions}
\label{part3}

In the previous sections, we knew the number of solutions present in the search set, which allows the estimation of the optimal number of iterations of the algorithm leading to a maximum probability of success upon measurement. However, it turns out that even when constrained to an unknown number of solutions, a quantum algorithm can still find a solution in $\mathcal{O}(\sqrt{N})$. \cite{brassard2002quantum}

\subsection{Lemma}
\label{Lemma}

Let $M$ be the (unknown) number of solutions and $\theta = \mathrm{arcsin}(\sqrt{M/N})$. Let $m \in \mathbb{N}$ and $j$ chosen uniformly in the interval $[\![0, m-1]\!]$.
\\
Grover's algorithm applied to the initial state $| \psi \rangle = \frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} |k\rangle$ returns a solution after $j$ iterations with a probability given by:
\[P_m = \frac{1}{2} - \frac{\mathrm{sin}(4\theta)}{4m\mathrm{sin(2\theta)}}\]
In particular, $P_m \geq 1/4$ for $m \geq 1/\mathrm{sin}(2\theta)$.
\\
\textit{Proof:} First, note that for any $\alpha, \beta \in \mathbb{R}$:
\[ \sum_{k=0}^{m-1}\mathrm{cos}(\alpha + 2\beta k) = \frac{\mathrm{sin}(m\beta)\mathrm{cos}(\alpha+(m-1)\beta)}{\mathrm{sin}(\beta)}\]
In particular, for $\alpha=\beta$, we have:
\begin{align}
\sum_{k=0}^{m-1}\mathrm{cos}((2k+1)\alpha) = \frac{\mathrm{sin}(2m\alpha)}{2\mathrm{sin}(\alpha)}
\label{eq:3}
\end{align}
Thus, knowing that the probability of obtaining a solution after $j$ iterations of Grover's algorithm is precisely $\mathrm{sin}^2((2j+1)\theta)$, for $J_m \sim \mathcal{U}(0,m-1)$ a random variable following a discrete uniform distribution on $[\![0,m-1]\!]$, we have by the transfer formula:
\begin{align*}
P_m =&\ \mathbb{E}(\mathrm{sin}^2((2J+1)\theta)) \\
=&\ \sum_{j=0}^{m-1}\mathrm{sin}^2((2j+1)\theta) \frac{1}{m} \\
=&\ \frac{1}{2m} \sum_{j=0}^{m-1}1 - \mathrm{cos}^2((2j+1)2\theta) \\
=&\ \frac{1}{2} - \frac{\mathrm{sin}(4\theta)}{4m\mathrm{sin(2\theta)}}
\end{align*}
According to identity \eqref{eq:3}. If $m \geq 1/\mathrm{sin}(2\theta)$, then:
\[\frac{\mathrm{sin}(4\theta)}{4m\mathrm{sin(2\theta)}} \leq \frac{1}{4m\mathrm{sin(2\theta)}} \leq \frac{1}{4}\]
Hence the result.

\subsection{Algorithm}
Assume initially that $1 \leq M \leq 3N/4$. Let $i$ be the iterator of the outer loop:
\begin{itemize}
	\item[1] Initialize $m_{(0)}=1$ and $\lambda \in ]1, 4/3[$.
	\item[2] Choose $j_{(i)}$ uniformly in $[\![0, m_{(i)}-1]\!]$.
	\item[3] Perform $j_{(i)}$ iterations of Grover's algorithm starting from the initial state $| \psi \rangle = \frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} |k\rangle$.
	\item[4] Measure the state vector $| \psi_{(j_{(i)})} \rangle$: Let $t$ be the result.
	\item[5]
	\begin{itemize}
    	\item[5.1] If $t$ is a solution: End of the algorithm.
    	\item[5.2] Otherwise: $m_{(i+1)} = \mathrm{min}(\lceil \lambda m_{(i)} \rceil, \sqrt{N})$ and return to step 2.
	\end{itemize}
\end{itemize}

\subsection{Complexity}
The above algorithm finds a solution with an expected complexity of $\mathcal{O}(\sqrt{N/M})$.
\\
\textit{Proof:} Let $\theta = \mathrm{arcsin}(\sqrt{M/N})$. We set: \[\tilde{m} = 1/\mathrm{sin}(2\theta) = \frac{N}{2\sqrt{M(N-M)}} < \sqrt{N/M}\]
\\
Here, we aim to estimate the expected number of applications of Grover's iteration. At the $i$-th iteration of the main loop, the value of $m_{(i)}$ is approximately $\lambda^{i-1}$ and the number $j_{(i)}$ of iterations is on average half of $m_{(i)}$.
We say that the algorithm reaches its \textit{critical state} as soon as $i$ exceeds $\lceil \mathrm{log}_{\lambda}\tilde{m} \rceil$. Note here that the value of $m_{(i)}$ exceeds $\tilde{m}$ after this stage.
\\
The expected number of iterations required to reach the critical state is bounded by:
\[\sum_{s=1}^{\lceil \mathrm{log}_{\lambda}\tilde{m} \rceil} \mathbb{E}(J_{\lceil \lambda^{s-1}\rceil}) \approx \frac{1}{2} \sum_{s=1}^{\lceil \mathrm{log}_{\lambda}\tilde{m} \rceil} \lambda^{s-1} < \frac{\lambda}{2\lambda-2}\tilde{m} = \mathcal{O}(\tilde{m})\]
Thus, if the algorithm finds a solution before reaching the critical state, it finds it in $\mathcal{O}(\tilde{m})$, therefore $\mathcal{O}(\sqrt{N/M})$.
\\
If the critical state is reached, each subsequent iteration finds a solution with a probability greater than $1/4$ by virtue of Lemma \ref{Lemma}.
We can thus represent the worst-case result of the algorithm after $\lceil \mathrm{log}_{\lambda}\tilde{m} \rceil$ iterations by a Bernoulli trial $X \sim \mathcal{B}(1/4)$ where the outcome of success means the algorithm has found a result.
Furthermore, let $I \sim \mathcal{G}(1/4)$ be a geometric distribution variable derived from $X$. It counts the number of iterations of the outer loop before obtaining a solution.
\\
Knowing that at the $i$-th iteration (after reaching the critical state) we have $j_{(i)}$ iterations of Grover's algorithm, with $j_{(i)}$ given by $J_{\lceil \lambda^{i-1}\rceil + \lceil \mathrm{log}_{\lambda}\tilde{m} \rceil}$, we deduce the expected total number of iterations of Grover's algorithm post-critical state:
\begin{align*}
\mathbb{E}(J_{\lceil \lambda^{I-1}\rceil + \lceil \mathrm{log}_{\lambda}\tilde{m} \rceil})
=&\ \sum_{i=1}^{\infty} \left( \frac{3}{4} \right) ^{i-1} \! \left( \frac{1}{4} \right) \mathbb{E}(J_{\lceil \lambda^{i-1}\rceil + \lceil \mathrm{log}_{\lambda}\tilde{m} \rceil}) \\
\approx&\ \frac{1}{2} \sum_{s=0}^{\infty} \frac{3^s}{4^{s+1}}\lambda^{s+\lceil \mathrm{log}_{\lambda}\tilde{m} \rceil} \\
<&\ \frac{\lambda}{8-6\lambda}\tilde{m} = \mathcal{O}(\tilde{m})
\end{align*}
\\
By summing the two bounds, we have a total complexity of
\[\frac{\lambda}{2\lambda-2}\tilde{m} + \frac{\lambda}{8-6\lambda}\tilde{m} = \mathcal{O}(\tilde{m}) = \mathcal{O}(\sqrt{N/M})\] for $0<M \leq 3N/4$.
\\
The case where $M > 3N/4$ is handled with a constant expected complexity by classical sampling. Finally, for $M = 0$, an appropriate timeout should be implemented to assert that no solution exists in a time of $\mathcal{O}(\sqrt{N})$. The failure probability can be made arbitrarily low when a solution does exist.